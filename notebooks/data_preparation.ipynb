{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ijson\n",
    "import jsonlines\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.colors import Hot\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from datashader.utils import export_image\n",
    "from datashader import transfer_functions as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff92d5",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5fd537",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/albertoscinetti/Desktop/Thesis/exploratory-data-analysis/digital-specimen.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60185d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load a partition of the data where latitude and longitude coordinates are present\n",
    "\n",
    "# data has been loaded in batches with counter to limit ingestion and to avoid overloading memory \n",
    "\n",
    "filtered_data = []\n",
    "counter = 0 \n",
    "\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for obj in reader:\n",
    "        if \"_source\" in obj and \"ods:hasEvents\" in obj[\"_source\"]:\n",
    "            for identification in obj[\"_source\"][\"ods:hasEvents\"]:\n",
    "                if \"ods:hasLocation\" in identification:\n",
    "                    location = identification[\"ods:hasLocation\"]\n",
    "                    if \"ods:hasGeoreference\" in location: \n",
    "                        georeference = location['ods:hasGeoreference']\n",
    "                        if \"dwc:decimalLatitude\" in georeference and \"dwc:decimalLongitude\" in georeference:\n",
    "                            counter += 1\n",
    "                            if counter > 1000000: # added this to skip the previously ingested data\n",
    "                                filtered_data.append(obj[\"_source\"])\n",
    "                                break\n",
    "        \n",
    "        if len(filtered_data) >= 500000:  # limit\n",
    "            break\n",
    "\n",
    "df = pd.json_normalize(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a1b38",
   "metadata": {},
   "source": [
    "### Standardisation of the df (extract values from nested columns, filter for only relevant columns...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting with these scripts all information related to the location of a speciic collecting events \n",
    "def extract_georeferences(row): \n",
    "    return row[0]['ods:hasLocation']['ods:hasGeoreference']\n",
    "\n",
    "def extract_latitude(row): \n",
    "    georeference_dict = row[0]['ods:hasLocation']['ods:hasGeoreference']\n",
    "    if 'dwc:decimalLatitude' in georeference_dict:\n",
    "        return georeference_dict['dwc:decimalLatitude']\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def extract_longitude(row): \n",
    "    georeference_dict = row[0]['ods:hasLocation']['ods:hasGeoreference']\n",
    "    if 'dwc:decimalLongitude' in georeference_dict: \n",
    "        return georeference_dict['dwc:decimalLongitude']\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def extract_country(row): \n",
    "    location_dict = row[0]['ods:hasLocation']\n",
    "    if 'dwc:country' in location_dict: \n",
    "        return location_dict['dwc:country']\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def extract_stateprovince(row): \n",
    "    location_dict = row[0]['ods:hasLocation']\n",
    "    if 'dwc:stateProvince' in location_dict: \n",
    "        return location_dict['dwc:stateProvince']\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def extract_localty(row): \n",
    "    location_dict = row[0]['ods:hasLocation']\n",
    "    if 'dwc:locality' in location_dict: \n",
    "        return location_dict['dwc:locality']\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def extract_island(row): \n",
    "    location_dict = row[0]['ods:hasLocation']\n",
    "    if 'dwc:island' in location_dict: \n",
    "        return location_dict['dwc:island']\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def extract_collector_name(row): \n",
    "    if 'ods:hasAgents' in row[0]:\n",
    "        agent_dict = row[0]['ods:hasAgents']\n",
    "        if 'schema:name' in agent_dict[0]:\n",
    "            return agent_dict[0]['schema:name']\n",
    "        else: \n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_collection_date(row): \n",
    "    if 'dwc:eventDate' in row[0]:\n",
    "        return row[0]['dwc:eventDate']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_georeference = df.copy()\n",
    "df_georeference['georeference'] = df_georeference['ods:hasEvents'].map(extract_georeferences)\n",
    "df_georeference['latitude'] = df_georeference['ods:hasEvents'].map(extract_latitude)\n",
    "df_georeference['longitude'] = df_georeference['ods:hasEvents'].map(extract_longitude)\n",
    "df_georeference['country'] = df_georeference['ods:hasEvents'].map(extract_country)\n",
    "df_georeference['stateProvince'] = df_georeference['ods:hasEvents'].map(extract_stateprovince)\n",
    "df_georeference['localty'] =  df_georeference['ods:hasEvents'].map(extract_localty)\n",
    "df_georeference['island'] =  df_georeference['ods:hasEvents'].map(extract_island)\n",
    "df_georeference['collector'] =  df_georeference['ods:hasEvents'].map(extract_collector_name)\n",
    "df_georeference['collection_date'] =  df_georeference['ods:hasEvents'].map(extract_collection_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_georeference[['@id','ods:sourceSystemName', 'ods:livingOrPreserved', 'ods:organisationName', 'ods:topicOrigin',\n",
    "       'ods:topicDomain', 'ods:topicDiscipline', 'ods:specimenName', 'latitude', 'longitude', 'country', 'stateProvince',\n",
    "       'localty', 'island', 'collector', 'collection_date'   ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6651c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('df_final_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "# the batches are put together to obtain a final df \n",
    "df1 = pd.read_csv('df_final_1.csv')\n",
    "df2 = pd.read_csv('df_final_2.csv')\n",
    "df3 = pd.read_csv('df_final_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df2['@id']) - set(df3['@id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a042d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine df1, df2, and df3 into a single DataFrame\n",
    "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "len(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('df_finalissimo.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load andcheck \n",
    "df_s2 = df_combined[['latitude', 'longitude']]\n",
    "df_s2.to_csv('s2_df_test_lat_long.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af29f3",
   "metadata": {},
   "source": [
    "### Data points visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obtain a visualization of the data points into a world map to understand the distribution \n",
    "\n",
    "df = df_combined.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# data shader in use here \n",
    "cvs = ds.Canvas(plot_width=1800, plot_height=900,\n",
    "                x_range=(-180, 180), y_range=(-90, 90))\n",
    "agg = cvs.points(df, 'longitude', 'latitude')\n",
    "img = tf.shade(agg, cmap=Hot, how='eq_hist')\n",
    "img = tf.spread(img, px=1)\n",
    "img_pil = img.to_pil().convert(\"RGBA\")\n",
    "\n",
    "# map plot \n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "ax.add_feature(cfeature.LAND.with_scale('110m'), facecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN.with_scale('110m'), facecolor='white')\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='white', linewidth=0.3)\n",
    "ax.add_feature(cfeature.BORDERS, edgecolor='white', linewidth=0.2, linestyle=':')\n",
    "\n",
    "# heatmap overlay\n",
    "ax.imshow(img_pil, origin='upper', extent=(-180, 180, -90, 90),\n",
    "          transform=ccrs.PlateCarree(), alpha=0.8)\n",
    "\n",
    "# plot \n",
    "plt.title(\"Global Data Point Density (Dark Map)\", color='white')\n",
    "ax.set_facecolor('black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035e6b4",
   "metadata": {},
   "source": [
    "### Convert Data into a textual format (pseudo sentences) for Ingestion into the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('specimens_with_adaptive_cells_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72397898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis#['ods:topicDiscipline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e83922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTAIN A DF WITH ONLY FREE TEXT FORM AND THEN THE CELL TOKEN \n",
    "def safe_fill(col):\n",
    "    return df_analysis[col].fillna(\"Unkn\")\n",
    "\n",
    "\n",
    "\n",
    "# the velow are commented out based on conducted analsysis and type of pseudosentences wanted to be extracted \n",
    "df_analysis['text_combined'] = (\n",
    "    \"Specimen: \" + safe_fill(\"ods:specimenName\") + \". \" +\n",
    "    #\"Collected by \" + safe_fill(\"collector\") + \" on \" + safe_fill(\"collection_date\") + \" in \" +\n",
    "    \"Collected in \" + safe_fill(\"localty\") + \", \" + safe_fill(\"stateProvince\") + \", \" + safe_fill(\"country\") + \". \"  # ADDED COLLECTECTED IN AND REMOVED COLLECTOR for an exepriment in this case\n",
    "    #\"Coordinates: (\" + df[\"latitude\"].fillna(\"Unknown\").astype(str) + \", \" +\n",
    "    #                  df[\"longitude\"].fillna(\"Unknown\").astype(str) + \"). \" +\n",
    "    # \"Discipline: \" + safe_fill(\"ods:topicDiscipline\") + \".\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['ods:topicDiscipline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa14040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_final = df_analysis[['text_combined',  'adaptive_cell_token', 'adaptive_cell_id', 'adaptive_cell_level' ]] #'country', 'stateProvince', 'localty',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d137b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na values for adavice cell toke columns \n",
    "df_analysis_final = df_analysis_final.dropna(subset=['adaptive_cell_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here also saved with locality as of now \n",
    "df_analysis_final.to_csv('df_for_model_nocoord_nocollector.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
